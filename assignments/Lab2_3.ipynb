{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6089405f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78975f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa7255",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263f9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity between 'car' and 'vehicle': 0.89\n",
      "Synsets for 'car':\n",
      "- car.n.01: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "- car.n.02: a wheeled vehicle adapted to the rails of railroad\n",
      "- car.n.03: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "- car.n.04: where passengers ride up and down\n",
      "- cable_car.n.01: a conveyance for passengers or freight on a cable railway\n",
      "\n",
      "Hyponyms (more specific terms) for 'car':\n",
      "- ambulance.n.01: a vehicle that takes people to and from hospitals\n",
      "- beach_wagon.n.01: a car that has a long body and rear door with space behind rear seat\n",
      "- bus.n.04: a car that is old and unreliable\n",
      "- cab.n.03: a car driven by a person whose job is to take passengers where they want to go in exchange for money\n",
      "- compact.n.03: a small and economical car\n",
      "- convertible.n.01: a car that has top that can be folded or removed\n",
      "- coupe.n.01: a car with two doors and front seats and a luggage compartment\n",
      "- cruiser.n.01: a car in which policemen cruise the streets; equipped with radiotelephonic communications to headquarters\n",
      "- electric.n.01: a car that is powered by electricity\n",
      "- gas_guzzler.n.01: a car with relatively low fuel efficiency\n",
      "- hardtop.n.01: a car that resembles a convertible but has a fixed rigid top\n",
      "- hatchback.n.01: a car having a hatchback door\n",
      "- horseless_carriage.n.01: an early term for an automobile\n",
      "- hot_rod.n.01: a car modified to increase its speed and acceleration\n",
      "- jeep.n.01: a car suitable for traveling over rough terrain\n",
      "- limousine.n.01: large luxurious car; usually driven by a chauffeur\n",
      "- loaner.n.02: a car that is lent as a replacement for one that is under repair\n",
      "- minicar.n.01: a car that is even smaller than a subcompact car\n",
      "- minivan.n.01: a small box-shaped passenger van; usually has removable seats; used as a family car\n",
      "- model_t.n.01: the first widely available automobile powered by a gasoline engine; mass-produced by Henry Ford from 1908 to 1927\n",
      "- pace_car.n.01: a high-performance car that leads a parade of competing cars through the pace lap and then pulls off the course\n",
      "- racer.n.02: a fast car that competes in races\n",
      "- roadster.n.01: an open automobile having a front seat and a rumble seat\n",
      "- sedan.n.01: a car that is closed and that has front and rear seats and two or four doors\n",
      "- sport_utility.n.01: a high-performance four-wheel drive car built on a truck chassis\n",
      "- sports_car.n.01: a small low car with a high-powered engine; usually seats two persons\n",
      "- stanley_steamer.n.01: a steam-powered automobile\n",
      "- stock_car.n.01: a car kept in dealers' stock for regular sales\n",
      "- subcompact.n.01: a car smaller than a compact car\n",
      "- touring_car.n.01: large open car seating four with folding top\n",
      "- used-car.n.01: a car that has been previously owned; not a new car\n",
      "\n",
      "Hypernyms (more general terms) for 'car':\n",
      "- motor_vehicle.n.01: a self-propelled wheeled vehicle that does not run on rails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aghaffar23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to calculate semantic similarity between two words\n",
    "def word_similarity(word1, word2):\n",
    "    # Get the synsets for each word\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return None\n",
    "    \n",
    "    max_similarity = 0.0\n",
    "    \n",
    "    # Calculate the similarity between synsets\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.wup_similarity(synset2)  # Wu-Palmer Similarity\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    \n",
    "    return max_similarity\n",
    "\n",
    "# Example usage\n",
    "word1 = \"car\"\n",
    "word2 = \"vehicle\"\n",
    "similarity = word_similarity(word1, word2)\n",
    "if similarity is not None:\n",
    "    print(f\"Semantic similarity between '{word1}' and '{word2}': {similarity:.2f}\")\n",
    "else:\n",
    "    print(f\"No synsets found for one or both words.\")\n",
    "\n",
    "# Example to find synonyms (synsets), hyponyms, and hypernyms for a word\n",
    "word = \"car\"\n",
    "synsets = wordnet.synsets(word)\n",
    "if synsets:\n",
    "    print(f\"Synsets for '{word}':\")\n",
    "    for synset in synsets:\n",
    "        print(f\"- {synset.name()}: {synset.definition()}\")\n",
    "    \n",
    "    # Hyponyms (more specific terms)\n",
    "    hyponyms = synsets[0].hyponyms()\n",
    "    if hyponyms:\n",
    "        print(f\"\\nHyponyms (more specific terms) for '{word}':\")\n",
    "        for hyponym in hyponyms:\n",
    "            print(f\"- {hyponym.name()}: {hyponym.definition()}\")\n",
    "    \n",
    "    # Hypernyms (more general terms)\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    if hypernyms:\n",
    "        print(f\"\\nHypernyms (more general terms) for '{word}':\")\n",
    "        for hypernym in hypernyms:\n",
    "            print(f\"- {hypernym.name()}: {hypernym.definition()}\")\n",
    "else:\n",
    "    print(f\"No synsets found for '{word}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95b855",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf5d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: car.n.01: a motor vehicle with four wheels; usually propelled by an internal combustion engine, Lemma Count: 71\n",
      "Synset: car.n.02: a wheeled vehicle adapted to the rails of railroad, Lemma Count: 2\n",
      "Synset: car.n.03: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant, Lemma Count: 0\n",
      "Synset: car.n.04: where passengers ride up and down, Lemma Count: 0\n",
      "Synset: cable_car.n.01: a conveyance for passengers or freight on a cable railway, Lemma Count: 0\n"
     ]
    }
   ],
   "source": [
    "# Get synsets for the word \"car\" as a noun ('n')\n",
    "car_synsets = wordnet.synsets('car', 'n')\n",
    "\n",
    "# Create a list of synsets along with their lemma counts\n",
    "synset_counts = [(synset, synset.lemmas()[0].count()) for synset in car_synsets]\n",
    "\n",
    "# Sort the synsets by lemma counts in descending order\n",
    "sorted_synsets = sorted(synset_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print synsets and their counts in descending order of frequency\n",
    "for synset, count in sorted_synsets:\n",
    "    print(f\"Synset: {synset.name()}: {synset.definition()}, Lemma Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1c243",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ede45bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity between sentences: 1.0\n",
      "Wup Similarity(T1, T2) = 0.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "# Function to calculate the semantic similarity between two sentences\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # Tokenize the sentences into words\n",
    "    words1 = word_tokenize(sentence1)\n",
    "    words2 = word_tokenize(sentence2)\n",
    "\n",
    "    # Get synsets for each word in the sentences\n",
    "    synsets1 = [wordnet.synsets(word) for word in words1]\n",
    "    synsets2 = [wordnet.synsets(word) for word in words2]\n",
    "\n",
    "    # Calculate the maximum similarity between pairs of synsets\n",
    "    max_similarity = 0.0\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            if synset1 and synset2:\n",
    "                similarity = max(s1.wup_similarity(s2) for s1 in synset1 for s2 in synset2)\n",
    "                if similarity is not None and similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "    \n",
    "    return max_similarity\n",
    "\n",
    "def wup(S1, S2):\n",
    "    return S1.wup_similarity(S2)\n",
    "\n",
    "def Resnik(S1, S2):\n",
    "    return S1.res_similarity(S2, genesis_ic)\n",
    "\n",
    "options = {0 : wup,\n",
    "           1 : Resnik,\n",
    "          }\n",
    "\n",
    "def preProcess(preprocess, sentence):\n",
    "    Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    stemmer = SnowballStemmer(\"english\")#we will avoid the stemming because it will give a pbm with sysnset search\n",
    "    \n",
    "    words = word_tokenize(sentence)\n",
    "    #words = [stemmer.stem(word) for word in words] \n",
    "    if preprocess:\n",
    "        words = [word.lower() for word in words if word.isalpha() and word not in Stopwords] #get rid of numbers and Stopwords\n",
    " \n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def word_similarity(w1,w2,num):\n",
    "    S1 = wn.synsets(w1)[0]\n",
    "    S2 = wn.synsets(w2)[0]\n",
    "#    print(w1,w2)\n",
    "#    print(S1,'\\n_________________\\n',S2)\n",
    "    if S1 and S2:\n",
    "       similarity = options[num](S1, S2)\n",
    "       if similarity:\n",
    "          return round(similarity,2)\n",
    "    return 0\n",
    "\n",
    "def Similarity(preprocess, T1, T2, num):\n",
    "    words1 = preProcess(preprocess, T1)\n",
    "    words2 = preProcess(preprocess, T2)\n",
    "    \n",
    "    tf = TfidfVectorizer(use_idf=True)\n",
    "    tf.fit_transform([' '.join(words1), ' '.join(words2)])\n",
    "\n",
    "    Idf = dict(zip(tf.get_feature_names_out(), tf.idf_))\n",
    "    \n",
    "    Sim = 0\n",
    "    Sim_score1 = 0\n",
    "    Sim_score2 = 0\n",
    "    \n",
    "    for w1 in words1:\n",
    "        Max = 0\n",
    "        for w2 in words2:\n",
    "            score = word_similarity(w1,w2,num)\n",
    "            if Max < score:\n",
    "               Max = score\n",
    "        Sim_score1 += Max*Idf[w1]\n",
    "    Sim_score1 /= sum([Idf[w1] for w1 in words1])\n",
    "    \n",
    "#     print(round(Sim_score1,2))\n",
    "    for w2 in words2:\n",
    "        Max = 0\n",
    "        for w1 in words1:\n",
    "            score = word_similarity(w1,w2,num)\n",
    "            if Max < score:\n",
    "               Max = score\n",
    "        Sim_score2 += Max*Idf[w2]\n",
    "        \n",
    "    Sim_score2 /= sum([Idf[w1] for w2 in words2])\n",
    "#     print(round(Sim_score2,2))\n",
    "\n",
    "    Sim = (Sim_score1+Sim_score2)/2\n",
    "    \n",
    "    return round(Sim,2)\n",
    "\n",
    "# Example sentences\n",
    "T1 = \"Students feel unhappy today about the class today.\"\n",
    "T2 = \"Many students felt concepts of class test relevant.\"\n",
    "\n",
    "# Calculate and print the semantic similarity\n",
    "similarity = sentence_similarity(T1, T2)\n",
    "similarity_wup = Similarity(1, T1, T2, 0)\n",
    "# similarity_resnik = Similarity(T1, T2, 1)\n",
    "print(f\"Semantic similarity between sentences: {round(similarity, 2)}\")\n",
    "print('Wup Similarity(T1, T2) =',similarity_wup)\n",
    "# print('Resnik Similarity(T1, T2) =',similarity_resnik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15a0c6",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d8f28d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aghaffar23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aghaffar23\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity with preprocessing: 1.00\n",
      "Semantic similarity without preprocessing: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Example sentences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_sentence(sentence, remove_stopwords=False, stem_words=False):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence.lower())  # Convert to lowercase\n",
    "\n",
    "    # Remove stopwords if specified\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem words if specified\n",
    "    if stem_words:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Calculate and print the semantic similarity with preprocessing\n",
    "T1_p = preprocess_sentence(T1, remove_stopwords=True, stem_words=True)\n",
    "T2_p = preprocess_sentence(T2, remove_stopwords=True, stem_words=True)\n",
    "T1_p_s = ' '.join(str(e) for e in T1_p)\n",
    "T2_p_s = ' '.join(str(e) for e in T2_p)\n",
    "similarity_with_preprocessing = sentence_similarity(T1_p_s, T2_p_s)\n",
    "print(f\"Semantic similarity with preprocessing: {similarity_with_preprocessing:.2f}\")\n",
    "\n",
    "# Calculate and print the semantic similarity without preprocessing\n",
    "similarity_without_preprocessing = sentence_similarity(T1, T2)\n",
    "print(f\"Semantic similarity without preprocessing: {similarity_without_preprocessing:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa172613",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "205a21b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FuzzyWuzzy similarity score between sentences before preprocess: 53\n",
      "FuzzyWuzzy similarity score between sentences after preprocess: 60\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Function to preprocess and lemmatize a sentence\n",
    "def preprocess_and_lemmatize(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence.lower())  # Convert to lowercase\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize using WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the lemmatized words into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Example sentences\n",
    "T1 = \"Students feel unhappy today about the class today.\"\n",
    "T2 = \"Many students felt concepts of class test relevant.\"\n",
    "\n",
    "similarity_score = fuzz.ratio(T1, T2)\n",
    "\n",
    "# Preprocess and lemmatize the sentences\n",
    "preprocessed_T1 = preprocess_and_lemmatize(T1)\n",
    "preprocessed_T2 = preprocess_and_lemmatize(T2)\n",
    "\n",
    "# Calculate the FuzzyWuzzy similarity score\n",
    "similarity_score_p = fuzz.ratio(preprocessed_T1, preprocessed_T2)\n",
    "print(f\"FuzzyWuzzy similarity score between sentences before preprocess: {similarity_score}\")\n",
    "print(f\"FuzzyWuzzy similarity score between sentences after preprocess: {similarity_score_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6e7a8",
   "metadata": {},
   "source": [
    "# Question 6 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e39eacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "words_T1 = word_tokenize(T1.lower())  # Convert to lowercase\n",
    "words_T2 = word_tokenize(T2.lower())  # Convert to lowercase\n",
    "\n",
    "# Function to calculate the average word embedding vector for a sentence\n",
    "def average_embedding(sentence_words, model):\n",
    "    vectors = [model[word] for word in sentence_words if word in model]\n",
    "    if not vectors:\n",
    "        return None\n",
    "    return sum(vectors) / len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89317397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = gensim.models.fasttext.load_facebook_vectors(datapath('C:\\\\Users\\\\aghaffar23\\\\OneDrive - Oulun yliopisto\\\\Work\\\\Uni\\\\Courses\\\\NLP\\\\codes\\\\cc.en.300.bin\\\\cc.en.300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b926cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between sentences using fasttext model: 0.8119011127826077\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word embedding vectors for the sentences\n",
    "embedding_T1 = average_embedding(words_T1, model_fasttext)\n",
    "embedding_T2 = average_embedding(words_T2, model_fasttext)\n",
    "\n",
    "# Calculate cosine similarity between the two sentence embeddings\n",
    "embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "# similarity_score = np.dot(embedding_T1, embedding_T2) / (np.linalg.norm(embedding_T1) * np.linalg.norm(embedding_T2))\n",
    "similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "print(f\"Cosine similarity between sentences using fasttext model: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c41fa735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I ran python -m  gensim.scripts.glove2word2vec -i glove.6B.300d.txt -o glove.6B.300d.word2vec.txt code to convert txt file to the txt file below to read it with this command\n",
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format('C:\\\\Users\\\\aghaffar23\\\\OneDrive - Oulun yliopisto\\\\Work\\\\Uni\\\\Courses\\\\NLP\\\\codes\\\\glove.6B.300d.word2vec.txt', binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "647a7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between sentences using Glove model: 0.8663384252910884\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word embedding vectors for the sentences\n",
    "embedding_T1 = average_embedding(words_T1, model_glove)\n",
    "embedding_T2 = average_embedding(words_T2, model_glove)\n",
    "\n",
    "embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "\n",
    "# Calculate cosine similarity between the two sentence embeddings\n",
    "similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "print(f\"Cosine similarity between sentences using Glove model: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b0056b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = gensim.models.KeyedVectors.load_word2vec_format('C:\\\\Users\\\\aghaffar23\\\\OneDrive - Oulun yliopisto\\\\Work\\\\Uni\\\\Courses\\\\NLP\\\\codes\\\\glove.6B.300d.word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1a251dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between sentences using Word2vec model: 0.8663384252910884\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word embedding vectors for the sentences\n",
    "embedding_T1 = average_embedding(words_T1, model_word2vec)\n",
    "embedding_T2 = average_embedding(words_T2, model_word2vec)\n",
    "\n",
    "embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "\n",
    "# Calculate cosine similarity between the two sentence embeddings\n",
    "similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "print(f\"Cosine similarity between sentences using Word2vec model: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b907652",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21032c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Similarities with different metrics for all of the questions are stored in sim dictionary just first 10 of list we show:\n",
      "[0.9042288550402525, 0.8826061661557302, 0.8826061661557302, 65, 65, 1.0]\n",
      "[0.7958676359362555, 0.8999295536947399, 0.8999295536947399, 55, 57, 1.0]\n",
      "[0.5380886783321475, 0.7154154328444283, 0.7154154328444283, 28, 24, 1.0]\n",
      "[0.7987387016393281, 0.8481633348352544, 0.8481633348352544, 45, 43, 1.0]\n",
      "[0.9240177045349875, 0.9499700800375436, 0.9499700800375436, 66, 64, 1.0]\n",
      "[0.6253272739721533, 0.6715399182452603, 0.6715399182452603, 22, 26, 0.6666666666666666]\n",
      "[0.9643473296476769, 0.9699451765344302, 0.9699451765344302, 59, 79, 1.0]\n",
      "[0.7392092219678874, 0.9087268318739488, 0.9087268318739488, 85, 68, 1.0]\n",
      "[0.8146498289631632, 0.854215765124131, 0.854215765124131, 51, 50, 1.0]\n",
      "[0.8083993017154593, 0.733711311109538, 0.733711311109538, 36, 40, 0.7692307692307693]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate the average word embedding vector for a sentence\n",
    "def average_embedding(sentence_words, model):\n",
    "    vectors = [model[word] for word in sentence_words if word in model]\n",
    "    if not vectors:\n",
    "        return None\n",
    "    return sum(vectors) / len(vectors)\n",
    "\n",
    "csv_file = 'questions.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Replace 'column1' and 'column2' with the actual column names you want to read\n",
    "column1_name = 'question1'\n",
    "column2_name = 'question2'\n",
    "\n",
    "# Extract the columns as lists of strings\n",
    "sentences1_list = df[column1_name].astype(str).tolist()\n",
    "sentences2_list = df[column2_name].astype(str).tolist()\n",
    "\n",
    "sim = {}\n",
    "for indx, T1 in enumerate(sentences1_list):\n",
    "#     if indx>10:\n",
    "#         break\n",
    "    T2 = sentences2_list[indx]\n",
    "    result1 = []\n",
    "    # Tokenize the sentences into words\n",
    "    words_T1 = word_tokenize(T1.lower())  # Convert to lowercase\n",
    "    words_T2 = word_tokenize(T2.lower())  # Convert to lowercase\n",
    "\n",
    "    # Calculate average word embedding vectors for the sentences\n",
    "    embedding_T1 = average_embedding(words_T1, model_fasttext)\n",
    "    embedding_T2 = average_embedding(words_T2, model_fasttext)\n",
    "\n",
    "    embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "    embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "\n",
    "    # Calculate cosine similarity between the two sentence embeddings\n",
    "    similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "    result1.append(similarity_score)\n",
    "#     print(f\"Cosine similarity between sentences using fasttext model: {similarity_score}\")\n",
    "\n",
    "    # Calculate average word embedding vectors for the sentences\n",
    "    embedding_T1 = average_embedding(words_T1, model_glove)\n",
    "    embedding_T2 = average_embedding(words_T2, model_glove)\n",
    "\n",
    "    embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "    embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "\n",
    "    # Calculate cosine similarity between the two sentence embeddings\n",
    "    similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "    result1.append(similarity_score)\n",
    "#     print(f\"Cosine similarity between sentences using Glove model: {similarity_score}\")\n",
    "\n",
    "    # Calculate average word embedding vectors for the sentences\n",
    "    embedding_T1 = average_embedding(words_T1, model_word2vec)\n",
    "    embedding_T2 = average_embedding(words_T2, model_word2vec)\n",
    "\n",
    "    embedding_T1 = {index: value for index, value in enumerate(embedding_T1)}\n",
    "    embedding_T2 = {index: value for index, value in enumerate(embedding_T2)}\n",
    "\n",
    "    # Calculate cosine similarity between the two sentence embeddings\n",
    "    similarity_score = gensim.matutils.cossim(embedding_T1, embedding_T2)\n",
    "    result1.append(similarity_score)\n",
    "#     print(f\"Cosine similarity between sentences using Word2vec model: {similarity_score}\")\n",
    "\n",
    "    similarity_score = fuzz.ratio(T1, T2)\n",
    "    result1.append(similarity_score)\n",
    "    # Preprocess and lemmatize the sentences\n",
    "    preprocessed_T1 = preprocess_and_lemmatize(T1)\n",
    "    preprocessed_T2 = preprocess_and_lemmatize(T2)\n",
    "\n",
    "    # Calculate the FuzzyWuzzy similarity score\n",
    "    similarity_score_p = fuzz.ratio(preprocessed_T1, preprocessed_T2)\n",
    "#     print(f\"FuzzyWuzzy similarity score between sentences before preprocess: {similarity_score}\")\n",
    "#     print(f\"FuzzyWuzzy similarity score between sentences after preprocess: {similarity_score_p}\")\n",
    "    result1.append(similarity_score_p)\n",
    "\n",
    "    similarity_score = sentence_similarity(T1, T2)\n",
    "#     print(f\"Semantic similarity using Mihalacea between sentences: {similarity_score}\")\n",
    "    result1.append(similarity_score)\n",
    "\n",
    "#     similarity_wup = Similarity(1, T1, T2, 0)\n",
    "#     result1.append(similarity_wup)\n",
    "#     print(f\"Semantic similarity using WUP between sentences: {similarity_wup}\")\n",
    "    sim[indx] = result1\n",
    "#     print(f\"Sentences No. {indx} is done!\")\n",
    "print(80*\"-\")\n",
    "print(\"Similarities with different metrics for all of the questions are stored in sim dictionary just first 10 of list we show:\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(sim[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
